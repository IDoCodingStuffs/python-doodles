{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 71549,
     "databundleVersionId": 8561470,
     "sourceType": "competition"
    },
    {
     "sourceId": 4958,
     "sourceType": "datasetVersion",
     "datasetId": 2847
    }
   ],
   "dockerImageVersionId": 30733,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RSNA 2024 Lumbar Spine Degenerative Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Starter Notebook for Pytorch and Deep learning techniques\n",
    "\n",
    "Using ResNET"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does this notebook contains?\n",
    "\n",
    "* Data organized in an understandable and easy to use way\n",
    "* A pretrained ResNET for inference\n",
    "\n",
    "I have tried creating a notebook where you can just plug your deep learning models and everything else is sorted. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-06T13:03:46.909305Z",
     "iopub.execute_input": "2024-06-06T13:03:46.909943Z",
     "iopub.status.idle": "2024-06-06T13:03:52.461859Z",
     "shell.execute_reply.started": "2024-06-06T13:03:46.90991Z",
     "shell.execute_reply": "2024-06-06T13:03:52.461038Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-19T07:24:01.905068Z",
     "start_time": "2024-06-19T07:24:01.841192Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# read data\n",
    "#train_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/'\n",
    "train_path = '../data/rsna-2024-lumbar-spine-degenerative-classification/'\n",
    "\n",
    "train  = pd.read_csv(train_path + 'train.csv')\n",
    "label = pd.read_csv(train_path + 'train_label_coordinates.csv')\n",
    "train_desc  = pd.read_csv(train_path + 'train_series_descriptions.csv')\n",
    "test_desc   = pd.read_csv(train_path + 'test_series_descriptions.csv')\n",
    "sub         = pd.read_csv(train_path + 'sample_submission.csv')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_desc.head(5)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train.head(5)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_desc.head(5)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to generate image paths based on directory structure\n",
    "import os\n",
    "def generate_image_paths(df, data_dir):\n",
    "    image_paths = []\n",
    "    for study_id, series_id in zip(df['study_id'], df['series_id']):\n",
    "        study_dir =  os.path.join(data_dir, str(study_id))\n",
    "        series_dir = os.path.join(study_dir, str(series_id))\n",
    "        images = os.listdir(series_dir)\n",
    "        image_paths.extend([os.path.join(series_dir, img) for img in images])\n",
    "    return image_paths\n",
    "\n",
    "# Generate image paths for train and test data\n",
    "train_image_paths = generate_image_paths(train_desc, f'{train_path}/train_images')\n",
    "test_image_paths = generate_image_paths(test_desc, f'{train_path}/test_images')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(train_desc)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(train_image_paths)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to open and display DICOM images\n",
    "def display_dicom_images(image_paths):\n",
    "    plt.figure(figsize=(15, 5))  # Adjust figure size if needed\n",
    "    for i, path in enumerate(image_paths[:3]):\n",
    "        ds = pydicom.dcmread(path)\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.imshow(ds.pixel_array, cmap=plt.cm.bone)\n",
    "        plt.title(f\"Image {i+1}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display the first three DICOM images\n",
    "display_dicom_images(train_image_paths)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T04:58:26.487236Z",
     "start_time": "2024-06-20T04:58:26.400179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Function to open and display DICOM images along with coordinates\n",
    "def display_dicom_with_coordinates(image_paths, label_df):\n",
    "    fig, axs = plt.subplots(1, len(image_paths), figsize=(18, 6))\n",
    "    \n",
    "    for idx, path in enumerate(image_paths):  # Display images\n",
    "        study_id = int(path.replace(\"\\\\\", \"/\").split('/')[-3])\n",
    "        series_id = int(path.replace(\"\\\\\", \"/\").split('/')[-2])\n",
    "        \n",
    "        # Filter label coordinates for the current study and series\n",
    "        filtered_labels = label_df[(label_df['study_id'] == study_id) & (label_df['series_id'] == series_id)]\n",
    "        \n",
    "        # Read DICOM image\n",
    "        ds = pydicom.dcmread(path)\n",
    "        \n",
    "        # Plot DICOM image\n",
    "        axs[idx].imshow(ds.pixel_array, cmap='gray')\n",
    "        axs[idx].set_title(f\"Study ID: {study_id}, Series ID: {series_id}\")\n",
    "        axs[idx].axis('off')\n",
    "        \n",
    "        # Plot coordinates\n",
    "        for _, row in filtered_labels.iterrows():\n",
    "            axs[idx].plot(row['x'], row['y'], 'ro', markersize=5)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load DICOM files from a folder\n",
    "def load_dicom_files(path_to_folder):\n",
    "    files = [os.path.join(path_to_folder, f) for f in os.listdir(path_to_folder) if f.endswith('.dcm')]\n",
    "    files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split('-')[-1]))\n",
    "    return files\n",
    "\n",
    "# Display DICOM images with coordinates\n",
    "study_id = \"100206310\"\n",
    "study_folder = f\"{train_path}/train_images/{study_id}\"\n",
    "\n",
    "image_paths = []\n",
    "for series_folder in os.listdir(study_folder):\n",
    "    series_folder_path = os.path.join(study_folder, series_folder)\n",
    "    dicom_files = load_dicom_files(series_folder_path)\n",
    "    if dicom_files:\n",
    "        image_paths.append(dicom_files[0])  # Add the first image from each series\n",
    "\n",
    "display_dicom_with_coordinates(image_paths, label)"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './data/rsna-2024-lumbar-spine-degenerative-classification//train_images/100206310'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 43\u001B[0m\n\u001B[0;32m     40\u001B[0m study_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/train_images/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudy_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     42\u001B[0m image_paths \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 43\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m series_folder \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(study_folder):\n\u001B[0;32m     44\u001B[0m     series_folder_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(study_folder, series_folder)\n\u001B[0;32m     45\u001B[0m     dicom_files \u001B[38;5;241m=\u001B[39m load_dicom_files(series_folder_path)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] The system cannot find the path specified: './data/rsna-2024-lumbar-spine-degenerative-classification//train_images/100206310'"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define function to reshape a single row of the DataFrame\n",
    "def reshape_row(row):\n",
    "    data = {'study_id': [], 'condition': [], 'level': [], 'severity': []}\n",
    "    \n",
    "    for column, value in row.items():\n",
    "        if column not in ['study_id', 'series_id', 'instance_number', 'x', 'y', 'series_description']:\n",
    "            parts = column.split('_')\n",
    "            condition = ' '.join([word.capitalize() for word in parts[:-2]])\n",
    "            level = parts[-2].capitalize() + '/' + parts[-1].capitalize()\n",
    "            data['study_id'].append(row['study_id'])\n",
    "            data['condition'].append(condition)\n",
    "            data['level'].append(level)\n",
    "            data['severity'].append(value)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Reshape the DataFrame for all rows\n",
    "new_train_df = pd.concat([reshape_row(row) for _, row in train.iterrows()], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the reshaped dataframe\n",
    "new_train_df.head(20)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Print columns in a neat way\n",
    "print(\"\\nColumns in new_train_df:\")\n",
    "print(\",\".join(new_train_df.columns))\n",
    "\n",
    "print(\"\\nColumns in label:\")\n",
    "print(\",\".join(label.columns))\n",
    "\n",
    "print(\"\\nColumns in test_desc:\")\n",
    "print(\",\".join(test_desc.columns))\n",
    "\n",
    "print(\"\\nColumns in sub:\")\n",
    "print(\",\".join(sub.columns))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Merge the dataframes on the common columns\n",
    "merged_df = pd.merge(new_train_df, label, on=['study_id', 'condition', 'level'], how='inner')\n",
    "# Merge the dataframes on the common column 'series_id'\n",
    "final_merged_df = pd.merge(merged_df, train_desc, on='series_id', how='inner')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Merge the dataframes on the common column 'series_id'\n",
    "final_merged_df = pd.merge(merged_df, train_desc, on=['series_id','study_id'], how='inner')\n",
    "# Display the first few rows of the final merged dataframe\n",
    "final_merged_df.head(5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_merged_df[final_merged_df['study_id'] == 100206310].sort_values(['x','y'],ascending = True)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_merged_df[final_merged_df['series_id'] == 1012284084].sort_values(\"instance_number\")"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, we can see what the data represents\n",
    "\n",
    "Series ID 1012284084 contains 60 images, and how each image maps to each level and condition"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter the dataframe for the given study_id and sort by instance_number\n",
    "filtered_df = final_merged_df[final_merged_df['study_id'] == 1013589491].sort_values(\"instance_number\")\n",
    "\n",
    "# Display the resulting dataframe\n",
    "filtered_df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sort final_merged_df by study_id, series_id, and series_description\n",
    "sorted_final_merged_df = final_merged_df[final_merged_df['study_id'] == 1013589491].sort_values(by=['series_id', 'series_description', 'instance_number'])\n",
    "sorted_final_merged_df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We see that, <br>\n",
    "Saggital T1 images map to Neural Foraminal Narrowing <br>\n",
    "Axial T2 images map to Subarticular Stenosis <br>\n",
    "Saggital T2/STIR map to Canal Stenosis <br>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the row_id column\n",
    "final_merged_df['row_id'] = (\n",
    "    final_merged_df['study_id'].astype(str) + '_' +\n",
    "    final_merged_df['condition'].str.lower().str.replace(' ', '_') + '_' +\n",
    "    final_merged_df['level'].str.lower().str.replace('/', '_')\n",
    ")\n",
    "\n",
    "# Create the image_path column\n",
    "final_merged_df['image_path'] = (\n",
    "    f'{train_path}/train_images/' + \n",
    "    final_merged_df['study_id'].astype(str) + '/' +\n",
    "    final_merged_df['series_id'].astype(str) + '/' +\n",
    "    final_merged_df['instance_number'].astype(str) + '.dcm'\n",
    ")\n",
    "\n",
    "# Note: Check image path, since there's 1 instance id, for 1 image, but there's many more images other than the ones labelled in the instance ID. \n",
    "\n",
    "# Display the updated dataframe\n",
    "final_merged_df.head(5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_merged_df[final_merged_df[\"severity\"] == \"Normal/Mild\"].value_counts().sum()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_merged_df[final_merged_df[\"severity\"] == \"Moderate\"].value_counts().sum()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the base path for test images\n",
    "base_path = f'{train_path}/test_images/'\n",
    "\n",
    "# Function to get image paths for a series\n",
    "def get_image_paths(row):\n",
    "    series_path = os.path.join(base_path, str(row['study_id']), str(row['series_id']))\n",
    "    if os.path.exists(series_path):\n",
    "        return [os.path.join(series_path, f) for f in os.listdir(series_path) if os.path.isfile(os.path.join(series_path, f))]\n",
    "    return []\n",
    "\n",
    "# Mapping of series_description to conditions\n",
    "condition_mapping = {\n",
    "    'Sagittal T1': {'left': 'left_neural_foraminal_narrowing', 'right': 'right_neural_foraminal_narrowing'},\n",
    "    'Axial T2': {'left': 'left_subarticular_stenosis', 'right': 'right_subarticular_stenosis'},\n",
    "    'Sagittal T2/STIR': 'spinal_canal_stenosis'\n",
    "}\n",
    "\n",
    "# Create a list to store the expanded rows\n",
    "expanded_rows = []\n",
    "\n",
    "# Expand the dataframe by adding new rows for each file path\n",
    "for index, row in test_desc.iterrows():\n",
    "    image_paths = get_image_paths(row)\n",
    "    conditions = condition_mapping.get(row['series_description'], {})\n",
    "    if isinstance(conditions, str):  # Single condition\n",
    "        conditions = {'left': conditions, 'right': conditions}\n",
    "    for side, condition in conditions.items():\n",
    "        for image_path in image_paths:\n",
    "            expanded_rows.append({\n",
    "                'study_id': row['study_id'],\n",
    "                'series_id': row['series_id'],\n",
    "                'series_description': row['series_description'],\n",
    "                'image_path': image_path,\n",
    "                'condition': condition,\n",
    "                'row_id': f\"{row['study_id']}_{condition}\"\n",
    "            })\n",
    "\n",
    "# Create a new dataframe from the expanded rows\n",
    "expanded_test_desc = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "expanded_test_desc.head(5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# change severity column labels\n",
    "#Normal/Mild': 'normal_mild', 'Moderate': 'moderate', 'Severe': 'severe'}\n",
    "final_merged_df['severity'] = final_merged_df['severity'].map({'Normal/Mild': 'normal_mild', 'Moderate': 'moderate', 'Severe': 'severe'})"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_data = expanded_test_desc\n",
    "train_data = final_merged_df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(train_data)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "# Define a function to check if a path exists\n",
    "def check_exists(path):\n",
    "    return os.path.exists(path)\n",
    "\n",
    "# Define a function to check if a study ID directory exists\n",
    "def check_study_id(row):\n",
    "    study_id = row['study_id']\n",
    "    path = f'{train_path}/train_images/{study_id}'\n",
    "    return check_exists(path)\n",
    "\n",
    "# Define a function to check if a series ID directory exists\n",
    "def check_series_id(row):\n",
    "    study_id = row['study_id']\n",
    "    series_id = row['series_id']\n",
    "    path = f'{train_path}/train_images/{study_id}/{series_id}'\n",
    "    return check_exists(path)\n",
    "\n",
    "# Define a function to check if an image file exists\n",
    "def check_image_exists(row):\n",
    "    image_path = row['image_path']\n",
    "    return check_exists(image_path)\n",
    "\n",
    "# Apply the functions to the train_data dataframe\n",
    "train_data['study_id_exists'] = train_data.apply(check_study_id, axis=1)\n",
    "train_data['series_id_exists'] = train_data.apply(check_series_id, axis=1)\n",
    "train_data['image_exists'] = train_data.apply(check_image_exists, axis=1)\n",
    "\n",
    "# Filter train_data\n",
    "train_data = train_data[(train_data['study_id_exists']) & (train_data['series_id_exists']) & (train_data['image_exists'])]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(train_data)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_data.head(3)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_dicom(path):\n",
    "    dicom = pydicom.read_file(path)\n",
    "    data = dicom.pixel_array\n",
    "    data = data - np.min(data)\n",
    "    if np.max(data) != 0:\n",
    "        data = data / np.max(data)\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load images randomly\n",
    "import random\n",
    "images = []\n",
    "row_ids = []\n",
    "selected_indices = random.sample(range(len(train_data)), 2)\n",
    "for i in selected_indices:\n",
    "    image = load_dicom(train_data['image_path'][i])\n",
    "    images.append(image)\n",
    "    row_ids.append(train_data['row_id'][i])\n",
    "\n",
    "# Plot images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for i in range(2):\n",
    "    ax[i].imshow(images[i], cmap='gray')\n",
    "    ax[i].set_title(f'Row ID: {row_ids[i]}', fontsize=8)\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.fromarray(images[0], 'L')\n",
    "img = img.resize((224, 224))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "images[1]"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
